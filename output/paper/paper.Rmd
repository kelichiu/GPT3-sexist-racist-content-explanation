---
output: 
  bookdown::pdf_document2:
    citation_package: natbib
    keep_tex: false
    toc: false
    fig_caption: true
    latex_engine: xelatex
    template: templates/svm-latex-ms.tex
bibliography: "references.bib"
header-includes:
  -  \usepackage{hyperref}
  -  \usepackage{amsmath}
biblio-style: apalike
title: "Explaining Why Text is Sexist or Racist with GPT-3"
thanks: "**Acknowledgements:**  We thank Monica Alexander and Thomas William Rosenthal for helpful comments. Comments on the `r format(Sys.time(), '%d %B %Y')` version of this paper are welcome at: rohan.alexander@utoronto.ca. **Funding statement:** We have no direct financial disclosures. We gratefully acknowledge the indirect support of Gillian Hadfield, the Schwartz Reisman Institute for Technology and Society, and OpenAI for providing free access to GPT-3. **Data accessibility statement:** Code and data are available at: https://github.com/kelichiu/GPT3-sexist-racist-content-explanation. We are unable to share direct access to GPT-3, however have provided the code that generated the GPT-3 output and all GPT-3 output. Please note that we have added asterisks to racial slurs and other offensive content in this paper, however the inputs and outputs did not have these. **Author contributions:** Chiu had the original idea, obtained and created the datasets, and wrote the first draft of the paper. Chiu and Alexander analysed and interpreted the data, contributed to writing the paper, and approved the final version."
author:
- name: Ke-Li Chiu
  affiliation: University of Toronto
- name: Rohan Alexander
  affiliation: University of Toronto
abstract: "Being able to generate a coherent explanation for an opinion is important for establishing trust in that opinion. We assess the extent to which GPT-3 can generate explanations for why a given text is sexist or racist. We find a general inadequacy in the explanations provided by GPT-3. Even when GPT-3 correctly classifies a statement as sexist or racist, it often provides false or partial explanations. Sensible explanations are a critical part of establishing trust in large language models, and AI more generally, and more work is needed in this space."
keywords: "GPT-3; natural language processing; quantitative analysis; hate speech."
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 12pt
endnote: no
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), echo = FALSE, warning = FALSE)
library(dplyr)
library(tidyverse)
library(tidyr)
library(readr)
```



# Introduction

**This paper contains language and themes that are offensive.**

Large language models such as OpenAI's GPT-3 can generate text that is indistinguishable from that created by humans. They can also classify whether some given text is sexist or racist [@chiu2021detecting]. As AI systems have improved over the past few decades, there has been increased interest in explanation [@mueller2019explanation]. We assess the extent to which GPT-3 can generate explanations for why a given text is sexist or racist. We are interested in GPT-3's ability to generate explanations because of the critical role this plays in establishing trust [@pieters2011explanation]. Trust is an essential factor for acceptance and shapes the ways in which humans and technology interact [@siau2018building]. 

We prompt GPT-3 to generate explanations in two ways: open-ended and structured. In the open-ended approach, we prompt GPT-3 in a question-and-answer manner: 'Is the following statement in quotes sexist? Answer yes or no and explain why. "[Statement here]"'. In the structured approach, we prompt GPT-3 to follow a given pattern. We then assess the adequacy of the explanations generated by GPT-3. We are interested in firstly, the extent to which it correctly classifies whether the statements are sexist/racist or not; and secondly, the reasonableness of the explanation that accompanies that classification.

We find that GPT-3 does poorly in the open-ended approach. When we add more structure to guide its responses the model performs better. But even when it correctly classifies racism or sexism, the accompanying explanations are rarely accurate. At times they even contradict the classification. On a technical level, we find a clear relationship between the hyper-parameter *temperature* and the number of correctly matched attributes, with substantial decreases as *temperature* increases.

Our approach to generating explanations is not 'Explainable AI'. That is an approach that aims to tackle the black box problem in AI decision-making by emphasising a process that is auditable and transparent [@arrieta2020explainable] and there are a variety of complications associated with this [@babic2021beware]. The explanations we prompt GPT-3 to produce do not speak to the technical or algorithmic process by which they were generated. Further, the model's classification of a statement as sexist or racist does not depend on the explanation it generates and can contradict it. That said, the generation of an explanation would overlap with explainable AI insofar as having potential to increase trust in an AI system [@hoffman2018metrics]. Therefore, sophisticated language models' fidelity of generating valid and truthful explanations is of lasting and critical importance.

The remainder of this paper is structured as follows: Section \@ref(background) provides a brief background on GPT-3 and its use in sexist or racist text detection. Section \@ref(method) provides details about the approach that we took. Section \@ref(results) contains our main findings. Finally, the implications of these findings, and some limitations, are discussed in Section \@ref(discussion).

# Background

## GPT-3

GPT-3 is the third generation of the Generative Pre-trained Transformer (GPT) model created by OpenAI [@brown2020language]. Compared to the language models that preceded it, GPT-3 has a few distinguishing features. Firstly, GPT-3 is a general model that can be applied to a variety of text-based tasks. Secondly, it can 'learn' by picking up patterns and producing results that follow those patterns. Thirdly, the examples and instructions fed to the model are expressed in natural language. In other words, it is programming with plain language instead of code. Finally, the model generates text quickly and fluently, and the generated content is often hard to distinguish from that created by humans. 

These characteristics mean researchers are wary that large language models could be used to generate misinformation at large scale [@bender2021dangers]. Large language models, such as Google's GShard [@lepikhin2020gshard] and Switch-C [@fedus2021switch] tend to be less accessible to the public. OpenAI grants selected access to GPT-3 through an API, while the two earlier versions, namely GPT and GPT-2, are publicly available.

## Sexist and racist text detection

@chiu2021detecting use GPT-3 to identify sexist and racist hate speech text passages with zero-, one-, and few-shot learning. They find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an accuracy between 48 per cent and 69 per cent. With few-shot learning and an instruction included in the prompt, the model's accuracy can be as high as 78 per cent. The implication of that paper is that large language models have a role to play in hate speech detection, and that with further development language models could be used to counter hate speech and even self-police. Similarly, @schick2021selfdiagnosis find language models are, to some extent, able to recognise undesirable bias.

## Comparison of human- and machine-generated explanations 

Using GPT-3 as the source of AI-generated text content, @ferguson2021Openforinterpretation compare AI-generated explanations of sexist comments with the explanations provided by humans. They collect sexist text from online forums, prompt GPT-3 to explain why a given text is sexist, and then compare the AI-generated explanations with those given in the forums. They find the model often provides relevant explanations that are similar to those given by humans. They also find that the themes of the explanations between AI and humans are similar. An important finding of @ferguson2021Openforinterpretation is that GPT-3 'tends to take men's position more often than women's', which may suggest inherited bias in large language models. 

The focus of @ferguson2021Openforinterpretation is sexism for a particular prompt. Instead, this paper examines both sexism and racism, and specifically compares different prompts. The structured approach in this paper could go beyond the scope of sexism or racism, to also consider discrimination based on disability, religion, appearance, and more. 

# Method

## ETHOS Hate Speech Dataset

The source of our racist and sexist statements is the ETHOS dataset, which is based on YouTube and Reddit comments [@mollas2020ethos]. The ETHOS YouTube data is collected through Hatebusters [@anagnostou2018hatebusters], which assigns a 'hate' score to each comment. The Reddit data is collected from the Public Reddit Data Repository [@baumgartner2020pushshift]. The ETHOS dataset has two variants: binary and multi-label. In the binary dataset statements are classified as hate or non-hate. In the multi-label variant, statements are evaluated on measures that include violence, gender, race, ability, religion, and sexual orientation. The statements that we use are as they exist in the ETHOS dataset and hence contain typos, misspelling, and offensive content.

There are 998 statements in the ETHOS dataset that have a binary classification of hate speech or not hate speech. Of these, the 433 statements that contain hate speech additionally have labels that further classify the content. For instance, whether the statements have to do with violence, gender, race, nationality, disability, etc. The label content is the proportion of annotators that positively classified the statement with that label and ranges between 0 and 1.

For the open-ended approach we qualitatively evaluate the results, so we only consider 76 unique statements: the 28 unique statements whose race-based score is 1, and the 48 unique statements whose gender-based score is 1. For the structured approach we consider 117 unique statements: the 66 unique statements whose race-based score is at least 0.75 and the 51 unique statements whose gender-based score is at least 0.75. For the structured approach, we reduce the threshold to 0.75 to include more statements in the dataset. This increased sample size is important in the structured approach because the evaluation of the results is more quantitative compared with the open-ended results whose evaluation is more qualitative. 

## Data collection

The data that we are interested in are the outputs of GPT-3. We prompt GPT-3 (DaVinci model) through the OpenAI API, and obtain three datasets: open-ended results, structured results with eight attributes, and structured results with six attributes. When generating the open-ended results, we prompt GPT-3 to produce five classifications and explanations per statement, which yields 380 generations in total. 

For the structured explanations, we use the API to generate two datasets, each consisting of 30 classifications and explanations for each of the 117 statements, for five different temperatures. The two datasets differ insofar as the number of attributes that we prompt GPT-3 to consider. One dataset has eight attributes: _target_, _targeting type_, _violence_, _derogatory_, _profanity_, _obscenity_, _rejection_, and _accusation_. The other dataset has only six: _target_, _targeting type_, _violence_, _insult_, _rejection_, and _accusation_. The latter dataset, where we combine _derogatory_, _profanity_, and _obscenity_ into _insult_, is a complementary dataset used to assess the model's capacity to recognize granular attributes (derogatory, profanity, and obscenity) compared with a more general attribute (insult). 

To evaluate the structured explanations, we primarily consider the dataset with eight attributes. There are 17,550 generations (117 unique statements each considered 30 times for each of five temperatures) in each of these two datasets.

### Hyper-parameters

We modify the following four hyper-parameters: _temperature_, _stop_, _n_, and _max_tokens_. 

The hyper-parameter _temperature_ determines how creative the response will be. The minimum value is 0 (conservative) and the maximum is 1 (creative). For open-ended responses, the temperature is set to 0.7, which is the default temperature in the OpenAI GPT-3 Playground. We choose to maintain the default setting, which enables some creative liberty to generate open-ended responses. On the other hand, for the structured responses, the temperature is iterated through 0, 0.25, 0.5, 0.75, and 1. We do not conduct grid tuning because we are not looking for the 'best' performance, but to compare the variation of performance under different temperature values. 

We set the _stop_ hyper-parameter to 'Q:', which means the generation will stop before the model generates another question in case the model follows the pattern and attempts to generate another similar question after the answer. 

The hyper-parameter _n_ is the number of responses generated, which we set to five. Because GPT-3's responses in an open-ended format can be very different each time, we prompt GPT-3 five times for each example to help us examine the variety of the responses and increase the chance of getting a reasonable response. 

The hyper-parameter _max_tokens_ is the maximum length of each response, which we set to 120. The goal of the prompt is to have the model answer 'yes' or 'no', and then generate free-form explanations comprising less than 120 tokens (a token is roughly equivalent to a word). 

### Prompts

The prompts that we use are formulated in a question-and-answer format. 

We start the prompt by 'Q: ' and place the question in a sentence to ask the model to explain why the provided statement is sexist/racist or not sexist/racist. We follow the question with the provided statement wrapped in quotes. We then end the prompt with 'A:', so the model will generate the response as an answer to the question. An example of an open-ended prompt can be found in Appendix \@ref(open-ended-prompts). This process is repeated until all the statements are classified as sexist or not, and racist or not, and accompanied by an explanation. The total number of collected responses in the open-ended format is 380, with five open-ended generations per statement.

For structured responses, the prompt contains two examples of sexist text and two examples of racist text. There is a particular desired pattern we want GPT-3 to follow, which is to answer 'yes' or 'no' to several pre-set attributes for each statement. An example of a structured prompt can be found in Appendix \@ref(structured-prompts). 

### Evaluation

The attributes that are of interest are: *target*, *targeting type*, *derogatory*, *profanity*, *obscenity*, *violence*, *rejection*, and *accusation*. We aim to evaluate GPT-3's ability to identify who the target is, what they are targeted based on, and the presence of the characteristics of abusive content. The *temperature* parameter is iterated through five different values---0, 0.25, 0.5, 0.75, and 1---as this changes the nature of the responses that we would expect. The *stop* parameter is set to ']', which is a closing bracket corresponding to the opening bracket in the prompt signaling the end of a generation. 

@chen2012detecting state that insulting language contains pejoratives, profanities, or obscenities. Therefore, the presence of insult can be seen as an attribute that encompasses derogatory, profanity and obscenity. We then repeated the process with _derogatory_, _profanity_ and _obscenity_ merged into a single attribute _insult_ to compare GPT-3's ability to detect granular attributes and a general attribute.

## Human evaluation

The model-generated responses are then evaluated by an annotator. This is done at different levels of specificity for the open-ended responses and the structured responses.

Three attributes of the open-ended responses are annotated: target identification; presence of made-up details; and reasoning adequacy. Target identification and presence of made-up details are binary evaluations that indicate if the model has identified the target of the sexist or racist content, and/or has made-up details in the generated explanations. The reasoning adequacy of each explanation is categorized as: no explanation; false explanation; partial explanation; and complete explanation.

For the structured responses, the annotator evaluates the given text by: target, targeting type, violence, derogatory, profanity, obscenity, rejection, and accusation. The guidelines for labeling are:

* Target: The annotator is free to list none, one, more multiple targets based on their judgment. Although the examples are focused on targets whose identities are based on gender or race, the annotator is not limited to these options.
* Targeting type: Similar to identifying targets, the annotator is also free to label the targeting type beyond to scope of gender or race.
* Violence: The annotator will only flag the presence of violence if the given text incites violence towards the target. Mentioning a violent event is not considered presence of violence.
* Derogatory: The annotator flags content that is belittling, demeaning, or degrading toward a target in a given text [@mwderogatory]. 
* Profanity: If a given text contains any swear words or curse words [@mwdprofanity], the presence of profanity should be labelled.
* Obscenity: Presence of obscenity should be flagged if a given text contains depiction of nudity, sex, or excretion, is patently offensive, and without artistic or scientific value.
* Rejection: The annotator will flag the presence of rejection if the given text contains the author's explicit expression of not wanting the presence or existence of a target [@fcc2021].
* Accusation: The author accuses their target of misconduct or mischievous behaviour with generalization without evidence.

# Results

## Open-ended responses

### Binary results

GPT-3 is prompted to identify whether a given text is sexist or racist by answering 'yes' or 'no' before generating an explanation. Table \@ref(tab:binaryresultagg) shows matches and mismatches of such identification. If GPT-3 answers 'yes', or in any other way indicates a text is sexist or racist, the answer is a match, regardless of the adequacy of the explanation. The number of samples are the aggregated counts of the five explanations for each comment. The results shown in Table \@ref(tab:binaryresultagg) indicate that GPT-3 did not successfully identify sexist or racist text content a majority of the time.

```{r binaryresultagg}
open_ended_results_raw <-
  read.csv(here::here("output/data/open_ended_binary_match.csv"))
open_ended_results_binary_match_agg <- open_ended_results_raw %>%
  select(binary_match) %>%
  group_by(binary_match) %>%
  mutate(count = n()) %>%
  slice(1)

open_ended_results_binary_match_agg <-
  open_ended_results_binary_match_agg %>%
  mutate(binary_match = factor(binary_match)) %>%
  mutate("Binary match" = ifelse(binary_match == 1, "Match", "Mismatch"))
open_ended_results_binary_match_agg <-
  open_ended_results_binary_match_agg[, c("Binary match", "count")]
colnames(open_ended_results_binary_match_agg) <-
  c("Binary match", "Count")
open_ended_results_binary_match_agg %>%
  knitr::kable(caption = "Open-ended result aggregated binary match count",
               booktabs = TRUE)
```

For each given text, we count the number of times GPT-3 successfully identifies it as sexist or racist, out of the set of five generated outputs. If, in three or more out of the five times, GPT-3 successfully classifies the text, it is marked 'majority correct'. For 37 out of the 76 statements, the model was majority-correct in terms of identifying sexist or racist statements (Table \@ref(tab:binaryresult)). 

```{r binaryresult}
open_ended_results_binary_match <- 
  open_ended_results_raw %>%
  select(binary_match, comment) %>%
  group_by(comment, binary_match) %>%
  mutate(count = n()) %>%
  slice(1) %>%
  mutate(
    majority_correct = case_when(
      binary_match == 0 & count > 2 ~ 0,
      binary_match == 0 & count < 3 ~ 1,
      binary_match == 1 & count > 2 ~ 1,
      binary_match == 1 & count < 3 ~ 0
    )
  ) %>%
  subset(select = -c(binary_match, count)) %>%
  unique() %>%
  subset(select = -c(comment)) %>%
  group_by(majority_correct) %>%
  mutate(count = n()) %>%
  slice(1)

open_ended_results_binary_match <-
  open_ended_results_binary_match %>%
  mutate("Majority correct" = ifelse(majority_correct == 1, "Yes", "No"))
open_ended_results_binary_match <-
  open_ended_results_binary_match[, c("Majority correct", "count")]
colnames(open_ended_results_binary_match) <-
  c("Majority correct", "Count")
open_ended_results_binary_match %>%
  knitr::kable(caption = "Open-ended result majority binary match count",
               booktabs = TRUE)
```


### Reasoning adequacy

For the 180 generations where GPT-3 correctly classified the sexist or racist text, we further examine the adequacy of the generated explanations. Table \@ref(tab:reasoningAdequacy) shows the count of the adequacy labels of the 180 evaluations. In 76 out of 180 times, the model's explanations contain false reasoning. In 60 cases, the model generates explanations with complete reasoning. When the reasoning the model provides is insufficient, or partially false, the reasoning is considered partial. In 30 cases, the model produces explanations with partial reasoning. In 14 cases, the model does not produce any explanation, or produces content that is not aimed at providing an explanation. 

```{r reasoningAdequacy}
open_ended_results_raw <-
  read.csv(here::here("output/data/open_ended_results_with_human_label.csv"))

# Reasoning level counts form evaluator 1
open_ended_results_reason_eval1 <- open_ended_results_raw %>%
  mutate_all(.funs=toupper) %>%
  filter(binary_match == 1) %>%
  group_by(eval1_reasoning) %>%
  mutate(eval1_count = n()) %>%
  slice(1) %>%
  select(eval1_reasoning, eval1_count)

colnames(open_ended_results_reason_eval1) <-
  c("Reasoning adequacy", "Count from evaluator 1")

# Reasoning level counts form evaluator 2
open_ended_results_reason_eval2 <- open_ended_results_raw %>%
  mutate_all(.funs=toupper) %>%
  filter(binary_match == 1) %>%
  group_by(eval2_reasoning) %>%
  mutate(eval2_count = n()) %>%
  slice(1) %>%
  select(eval2_reasoning, eval2_count)

colnames(open_ended_results_reason_eval2) <-
  c("Reasoning adequacy", "Count from evaluator 2")

# Merge raults from eval1 and eval2
open_ended_results_reason_eval1and2 <- merge(open_ended_results_reason_eval1, open_ended_results_reason_eval2, by = "Reasoning adequacy", sort = TRUE)
open_ended_results_reason_eval1and2 %>%
  knitr::kable(caption = "Open-ended result reasoning level counts",
               booktabs = TRUE)
```

### Target identification and made-up details

The 180 generations where GPT-3 successfully classified the sexist or racist text can be examined to see if the explanation provides an identification of who the target is and if the explanation is accompanied with false details. The results of target identification are displayed in Table \@ref(tab:openendedResultsTargetID). In 117 out of the 180 cases, the model provides an explanation along with the identification of the target (women, African Americans, ...). In some cases, the model generates explanations along with made-up, false, details such as assigning a false source to the given text. The number of cases where made-up details are found is shown in Table \@ref(tab:openendedResultsMadeupDetails). Nearly one-third of the time, the model generates made-up details in the explanations. 

```{r openendedResultsTargetID}

# Target identification count from evaluator 1
open_ended_results_targetID_eval1 <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(eval1_identify_target) %>%
  mutate(count = n()) %>%
  slice(1) %>%
  select(eval1_identify_target, count)
open_ended_results_targetID_eval1 <- open_ended_results_targetID_eval1 %>%
  mutate("Target identified" = ifelse(eval1_identify_target == 1, "Yes", "No"))
open_ended_results_targetID_eval1 <-
  open_ended_results_targetID_eval1[, c("Target identified", "count")]
colnames(open_ended_results_targetID_eval1) <-
  c("Target identified", "Count from evaluator 1")

# Target identification count from evaluator 2
open_ended_results_targetID_eval2 <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(eval2_identify_target) %>%
  mutate(count = n()) %>%
  slice(1) %>%
  select(eval2_identify_target, count)
open_ended_results_targetID_eval2 <- open_ended_results_targetID_eval2 %>%
  mutate("Target identified" = ifelse(eval2_identify_target == 1, "Yes", "No"))
open_ended_results_targetID_eval2 <-
  open_ended_results_targetID_eval2[, c("Target identified", "count")]
colnames(open_ended_results_targetID_eval2) <-
  c("Target identified", "Count from evaluator 2")

# Merge raults from eval1 and eval2
open_ended_results_targetID_eval1and2 <- merge(open_ended_results_targetID_eval1, open_ended_results_targetID_eval2, by = "Target identified", sort = TRUE)

open_ended_results_targetID_eval1and2 %>%
  knitr::kable(caption = "Open-ended result target identification counts",
               booktabs = TRUE)
```

```{r openendedResultsMadeupDetails}
# Made-up detail counts from evaluator 1
open_ended_results_madeup_details_eval1 <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(eval1_madeup_details) %>%
  mutate(count = n()) %>%
  slice(1) %>%
  select(eval1_madeup_details, count)

open_ended_results_madeup_details_eval1 <- 
  open_ended_results_madeup_details_eval1 %>% 
  mutate(eval1_madeup_details = ifelse(eval1_madeup_details == 1, "Yes", "No"))

colnames(open_ended_results_madeup_details_eval1) <-
  c("Made-up details", "Count from evaluator 1")

# Made-up detail counts from evaluator 2
open_ended_results_madeup_details_eval2 <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(eval2_madeup_details) %>%
  mutate(count = n()) %>%
  slice(1) %>%
  select(eval2_madeup_details, count)

open_ended_results_madeup_details_eval2 <- 
  open_ended_results_madeup_details_eval2 %>% 
  mutate(eval2_madeup_details = ifelse(eval2_madeup_details == 1, "Yes", "No"))

colnames(open_ended_results_madeup_details_eval2) <-
  c("Made-up details", "Count from evaluator 2")

# Merge raults from eval1 and eval2
open_ended_results_madeup_details_eval1and2 <- merge(open_ended_results_madeup_details_eval1, open_ended_results_madeup_details_eval2, by = "Made-up details", sort = TRUE)

open_ended_results_madeup_details_eval1and2 %>%
  knitr::kable(caption = "Open-ended result made-up details counts",
               booktabs = TRUE)
```

## Structured responses

```{r}
structured_a8_t0_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp0_attr8_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp0_attr8_n30.csv"
    )),
    by = "comment"
  )
structured_a8_t25_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp25_attr8_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp25_attr8_n30.csv"
    )),
    by = "comment"
  )
structured_a8_t50_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp50_attr8_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp50_attr8_n30.csv"
    )),
    by = "comment"
  )
structured_a8_t75_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp75_attr8_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp75_attr8_n30.csv"
    )),
    by = "comment"
  )
structured_a8_t100_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp1_attr8_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp1_attr8_n30.csv"
    )),
    by = "comment"
  )
structured_a6_t0_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp0_attr6_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp0_attr6_n30.csv"
    )),
    by = "comment"
  )
structured_a6_t25_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp25_attr6_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp25_attr6_n30.csv"
    )),
    by = "comment"
  )
structured_a6_t50_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp50_attr6_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp50_attr6_n30.csv"
    )),
    by = "comment"
  )
structured_a6_t75_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp75_attr6_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp75_attr6_n30.csv"
    )),
    by = "comment"
  )
structured_a6_t100_n30 <-
  merge(
    read.csv(here::here(
      "output/data/structured_n30_eval1/structured_results_temp1_attr6_n30.csv"
    )),
    read.csv(here::here(
      "output/data/structured_n30_eval2/structured_results_temp1_attr6_n30.csv"
    )),
    by = "comment"
  )

```
```{r}
count_mismatch <- function(df, column_to_count, column_prefix) {
  new_df <- df %>%
    filter(gender >= 0.75 | race >= 0.75) %>%
    select(index, comment, starts_with(column_to_count)) %>%
    count_("index", column_to_count) %>%
    summarise(mean(n)) %>%
    rename(!!column_prefix := "mean(n)")
  new_df
}
count_mismatch_temps <- function(column_to_count, attribute) {
  a8_t0 <-
    count_mismatch(structured_a8_t0_n30, column_to_count, "Temperature 0")
  a8_t25 <-
    count_mismatch(structured_a8_t25_n30, column_to_count, "Temperature 25")
  a8_t50 <-
    count_mismatch(structured_a8_t50_n30, column_to_count, "Temperature 50")
  a8_t75 <-
    count_mismatch(structured_a8_t75_n30, column_to_count, "Temperature 75")
  a8_t100 <-
    count_mismatch(structured_a8_t100_n30, column_to_count, "Temperature 100")
  mismatch_summary <-
    add_column(
      Attribute = attribute,
      a8_t0,
      a8_t25,
      a8_t50,
      a8_t75,
      a8_t100
    )
  mismatch_summary <-
    mismatch_summary[, c(
      "Attribute",
      "Temperature 0",
      "Temperature 25",
      "Temperature 50",
      "Temperature 75",
      "Temperature 100"
    )]
  colnames(mismatch_summary) <-
    c(
      "Attribute",
      "Temperature 0",
      "Temperature 0.25",
      "Temperature 0.5",
      "Temperature 0.75",
      "Temperature 1"
    )
  mismatch_summary
}
```


```{r mismatchSummary}
### Target
target_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_target", "Target (eval1)"),
        count_mismatch_temps("eval2_matched_target", "Target (eval2)"))

### Targeting Type
targetingtype_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_targeting_type", "Targeting Type (eval1)"),
        count_mismatch_temps("eval2_matched_targeting_type", "Targeting Type (eval2)")
        )

### Violence
violence_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_violence", "Violence (eval1)"),
        count_mismatch_temps("eval2_matched_violence", "Violence (eval2)"))

### Derogatory
derogatory_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_derogatory", "Derogatory (eval1)"),
        count_mismatch_temps("eval2_matched_derogatory", "Derogatory (eval2)"))

### Profanity
profanity_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_profanity", "Profanity (eval1)"),
        count_mismatch_temps("eval2_matched_profanity", "Profanity (eval2)"))

### Obscenity
obscenity_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_obscenity", "Obscenity (eval1)"),
        count_mismatch_temps("eval2_matched_obscenity", "Obscenity (eval2)"))

### Rejection
rejection_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_rejection", "Rejection (eval1)"),
        count_mismatch_temps("eval2_matched_rejection", "Rejection (eval2)"))

### Accusation
accusation_mismatch_summary <- 
  rbind(count_mismatch_temps("eval1_matched_accusation", "Accusation (eval1)"),
        count_mismatch_temps("eval2_matched_accusation", "Accusation (eval2)"))
```

```{r}
merge_df_30n <- function(df, temp) {
  new_df <- df %>%
    filter(gender >= 0.75 | race >= 0.75) %>%
    group_by(index) %>%
    summarise_if(is.numeric, sum, na.rm = TRUE)  %>%
    select(index, starts_with("eval")) %>%
    mutate(temperature = temp)
  new_df
}
structured_a8_t0_n30_list <- 
  merge_df_30n(structured_a8_t0_n30, 0)
structured_a8_t25_n30_list <-
  merge_df_30n(structured_a8_t25_n30, 0.25)
structured_a8_t50_n30_list <-
  merge_df_30n(structured_a8_t50_n30, 0.5)
structured_a8_t75_n30_list <-
  merge_df_30n(structured_a8_t75_n30, 0.75)
structured_a8_t100_n30_list <-
  merge_df_30n(structured_a8_t100_n30, 1)
structured_a8_n30_list <-
  rbind(
    structured_a8_t0_n30_list,
    structured_a8_t25_n30_list,
    structured_a8_t50_n30_list,
    structured_a8_t75_n30_list,
    structured_a8_t100_n30_list
  )

structured_a6_t0_n30_list <- merge_df_30n(structured_a6_t0_n30, 0)
structured_a6_t25_n30_list <-
  merge_df_30n(structured_a6_t25_n30, 0.25)
structured_a6_t50_n30_list <-
  merge_df_30n(structured_a6_t50_n30, 0.5)
structured_a6_t75_n30_list <-
  merge_df_30n(structured_a6_t75_n30, 0.75)
structured_a6_t100_n30_list <-
  merge_df_30n(structured_a6_t100_n30, 1)
structured_a6_n30_list <-
  rbind(
    structured_a6_t0_n30_list,
    structured_a6_t25_n30_list,
    structured_a6_t50_n30_list,
    structured_a6_t75_n30_list,
    structured_a6_t100_n30_list
  )
```

### Effect of temperature

Figure \ref{fig:avgmatches} shows the structured result average number of attribute matches for temperature values of: 0, 0.25, 0.5, 0.75, and 1. We observe a general trend that the number of matches decreases as the temperature increases. Except for _target_, _profanity_ and _obscenity_ where the model performs the best under temperature 0.25 by a small margin, the results of temperature 0 have the highest number of matches with the human evaluation in every other attribute. The model consistently performs poorly with temperature value of 1. Moreover, the distribution of the matched number of statements has a wider range at higher temperatures.

```{r avgmatches_eval1, fig.height=7, fig.cap="Average number of matched attributes in temperature 0, 0.25, 0.5, 0.75 and 1 (note the differing y-axes) from evaluator 1"}
p1 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_target)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Target") +
  theme_classic()

p2 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_targeting_type)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Type") +
  theme_classic()

p3 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_violence)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Violence") +
  theme_classic()

p4 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_derogatory)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Derogatory") +
  theme_classic()

p5 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_profanity)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Profanity") +
  theme_classic()

p6 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_obscenity)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Obscenity") +
  theme_classic()

p7 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_rejection)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Rejection") +
  theme_classic()

p8 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval1_matched_accusation)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Accusation") +
  theme_classic()

gridExtra::grid.arrange(
  p1,
  p2,
  p3,
  p4,
  p5,
  p6,
  p7,
  p8,
  ncol = 2,
  top = grid::textGrob("Average number of matched attributes in different temperatures from evaluator 1")
)
```

```{r avgmatches_eval2, fig.height=7, fig.cap="Average number of matched attributes in temperature 0, 0.25, 0.5, 0.75 and 1 (note the differing y-axes) from evaluator 2"}
p1 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_target)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Target") +
  theme_classic()

p2 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_targeting_type)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Type") +
  theme_classic()

p3 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_violence)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Violence") +
  theme_classic()

p4 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_derogatory)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Derogatory") +
  theme_classic()

p5 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_profanity)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Profanity") +
  theme_classic()

p6 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_obscenity)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Obscenity") +
  theme_classic()

p7 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_rejection)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Rejection") +
  theme_classic()

p8 <-
  ggplot(structured_a8_n30_list, aes(factor(temperature), eval2_matched_accusation)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, height = 0, alpha = 0.2) +
  labs(x = "Temperature",
       y = "Accusation") +
  theme_classic()

gridExtra::grid.arrange(
  p1,
  p2,
  p3,
  p4,
  p5,
  p6,
  p7,
  p8,
  ncol = 2,
  top = grid::textGrob("Average number of matched attributes in different temperatures from evaluator 2")
)
```


### Comparison of all attributes

The descriptive statistics shown in Table \@ref(tab:matchSummaryTable) assess overall performance in each attribute. The model performs the best in identifying the identity categories (sex, gender, race, origin, ...) that a sexist/racist statement is targeting. The model also performs well in identifying who the target is (women, African Americans, ...). The model seems to have varied capabilities in identifying the presence of violence, derogatory, profanity, obscenity, rejection, and accusation in the statements. 

```{r matchSummaryTable}
all_attr_summary <- add_row(target_mismatch_summary,targetingtype_mismatch_summary) %>%
  add_row(violence_mismatch_summary) %>%
  add_row(derogatory_mismatch_summary)%>%
  add_row(profanity_mismatch_summary)%>%
  add_row(obscenity_mismatch_summary)%>%
  add_row(rejection_mismatch_summary)%>%
  add_row(accusation_mismatch_summary)
colnames(all_attr_summary) <- c("Attribute", "Temp 0", "Temp 0.25", "Temp 0.5", "Temp 0.75", "Temp 1")
rownames(all_attr_summary) <- all_attr_summary$Attribute

all_attr_summary_t <- t(all_attr_summary)

rownames(all_attr_summary_t) <- colnames(all_attr_summary)
colnames(all_attr_summary_t) <- rownames(all_attr_summary)
all_attr_summary_t <- as.data.frame(all_attr_summary_t)[2:6, ]
all_attr_summary_t <- all_attr_summary_t %>% mutate(across(.fns = type.convert))

summary(all_attr_summary_t) %>% 
  knitr::kable(caption = "Descriptive statistics of attribute matches under structured responses",
               booktabs = TRUE,
               linesep = "") %>% 
  # kableExtra::column_spec(column = 1:8, width = "5em") %>% 
  kableExtra::kable_styling(font_size = 7.5)
```

### Generality of attributes

The general attribute insult encompasses derogatory, profanity, and obscenity, which are granular attributes. We examine how GPT-3 performs in identifying a more general attribute compared to identifying more granular attributes. Figure \ref{fig:insultMismatchSummaryLineChart} shows the average numbers of matches in _insult_, _derogatory_, _profanity_, and _obscenity_ with temperature values of: 0, 0.25, 0.5, 0.75 and 1. The results show that the model tends to perform better at identifying general attributes compared to granular attributes. 


```{r insultMismatchSummaryLineChartEval1, fig.height=3, fig.cap="Average number of matches in insult, derogatory, profanity, and obscenity under temperature 0, 0.25, 0.5, 0.75 and 1 from evaluator 1"}
insult_df <-
  add_column(
    structured_a6_n30_list["index"],
    structured_a6_n30_list["temperature"],
    structured_a6_n30_list["eval1_matched_insult"],
    structured_a8_n30_list["eval1_matched_derogatory"],
    structured_a8_n30_list["eval1_matched_profanity"],
    structured_a8_n30_list["eval1_matched_obscenity"]
  )

insult_df <-
  insult_df %>%
  group_by(index, temperature) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE) %>%
  group_by(temperature) %>%
  select(-index) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  pivot_longer(!temperature, names_to = "matched_attribute", values_to = "count") %>%
  mutate(matched_attribute = fct_relevel(
    matched_attribute,
    c(
      "eval1_matched_insult",
      "eval1_matched_derogatory",
      "eval1_matched_profanity",
      "eval1_matched_obscenity"
    )
  ))  %>%
  mutate(general = ifelse(matched_attribute == "eval1_matched_insult", "1", "0")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval1_matched_insult", "Matched Insult")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval1_matched_derogatory", "Matched Derogatory")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval1_matched_profanity", "Matched Profanity")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval1_matched_obscenity", "Matched Obscenity"))

ggplot(data = insult_df, aes(x = temperature, y = count, group = matched_attribute)) +
  geom_line(aes(color = matched_attribute)) +
  geom_point() +
  labs(
    title = "Average number of matches in insult, derogatory, profanity, and obscenity from evaluator 1",
    y = "Number",
    x = "Temperature",
    color = "Attribute"
  ) +
  theme_classic()
```
```{r insultMismatchSummaryLineChartEval2, fig.height=3, fig.cap="Average number of matches in insult, derogatory, profanity, and obscenity under temperature 0, 0.25, 0.5, 0.75 and 1 from evaluator 2"}
insult_df <-
  add_column(
    structured_a6_n30_list["index"],
    structured_a6_n30_list["temperature"],
    structured_a6_n30_list["eval2_matched_insult"],
    structured_a8_n30_list["eval2_matched_derogatory"],
    structured_a8_n30_list["eval2_matched_profanity"],
    structured_a8_n30_list["eval2_matched_obscenity"]
  )

insult_df <-
  insult_df %>%
  group_by(index, temperature) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE) %>%
  group_by(temperature) %>%
  select(-index) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  pivot_longer(!temperature, names_to = "matched_attribute", values_to = "count") %>%
  mutate(matched_attribute = fct_relevel(
    matched_attribute,
    c(
      "eval2_matched_insult",
      "eval2_matched_derogatory",
      "eval2_matched_profanity",
      "eval2_matched_obscenity"
    )
  ))  %>%
  mutate(general = ifelse(matched_attribute == "eval2_matched_insult", "1", "0")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval2_matched_insult", "Matched Insult")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval2_matched_derogatory", "Matched Derogatory")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval2_matched_profanity", "Matched Profanity")) %>%
  mutate(matched_attribute = str_replace(matched_attribute, "eval2_matched_obscenity", "Matched Obscenity"))

ggplot(data = insult_df, aes(x = temperature, y = count, group = matched_attribute)) +
  geom_line(aes(color = matched_attribute)) +
  geom_point() +
  labs(
    title = "Average number of matches in insult, derogatory, profanity, and obscenity from evaluator 2",
    y = "Number",
    x = "Temperature",
    color = "Attribute"
  ) +
  theme_classic()
```

# Discussion

## GPT-3 generally struggles to provide explanations

Without examples in the prompt, we find GPT-3 has difficulty classifying a statement as sexist or racist. This is similar to @chiu2021detecting who found GPT-3 did poorly at detecting sexist or racist content under zero-shot learning. More than half the time, GPT-3 incorrectly categorized statements as not-sexist or not-racist. The following is an example of a sexist statement identified by GPT-3 as not-sexist (asterisks added *ex post*):

> Prompt:
  
> Is the following statement in quotes sexist? Answer yes or no and explain why. '''I wouldn't like this girl as a police officer. I want police to keep me safe, she and every other woman cannot keep us safe they are week''' 

> GPT-3 response:

> No. The statement is not sexist because no two sexes are involved. The statement could be changed to "Don't hit Men. Just f*** them and make them pregnant." and be equally valid.

In the above generation, the model demonstrates its ability to generate coherent content and even swaps the object in the original to another gender. However, it does not successfully identify the sexism in the given content and the reasoning is not logical. 

In some cases, the model does not clearly state if a given statement is sexist or racist, but generates an answer that is ambiguous. The following is another generation provided by GPT-3 to that same prompt:

> Prompt:

> Is the following statement in quotes sexist? Answer yes or no and explain why. '''I wouldn't like this girl as a police officer. I want police to keep me safe, she and every other woman cannot keep us safe they are week''' 

> GPT-3 response:

> It depends on your definition of sexism. If you think that the statement is attributing an inferiority to women that does not exist and is just a result of sexist bias, then the statement is sexist. If you do not think this statement is sexist then you would answer no.

At other times, GPT-3 successfully identified sexist or racist content with a clear 'yes' or 'it is sexist/racist', but then the explanations that followed are false, partially false, or insufficient. In the following example, the model wrongly attributes the racist element to the use of 'you', and overlooks the offensive language that is directly attacking a target based on their race:

> Prompt:

> Is the following statement in quotes racist? Answer yes or no and explain why. '''U SUCK S*** YOU BLACK F***'''

> GPT-3 response:

> Yes. The statement is racist because it uses the word "you". This is an insult directed at blacks. It implies that all blacks are the same.

We also observe cases where GPT-3 generates explanations that contradict its initial identification. For instance:

> Prompt:

> Is the following statement in quotes sexist? Answer yes or no and explain why. '''I don't hate women i just think that they have nothing interesting to say. None reason for existance.''' 

> GPT-3 response:

> No, because it is sexist to suggest that women have nothing interesting to say. And it is sexist to suggest that women have no reason for existence.

In the above example, GPT-3 first categorizes the given statement as non-sexist. However, the following explanation states that the given statement is, in fact, sexist. The contradicting identification and explanation demonstrates that the classification is not based on the explanation, thus, the generation process that we are interested in should not be considered 'Explainable AI'. 

The model's explanations are at times accompanied with made-up details. In the example below, the model generated a valid and complete reasoning of why the given statement is sexist. However, it has made-up the source and the context of the given statement, thus affecting the credibility of the explanation. 

> Prompt:

> Is the following statement in quotes sexist? Answer yes or no and explain why. '''Women are made for making babies and cooking dinner and nothing else!!!'''

> GPT-3 response:

> Yes, it is sexist. The statement, which was made by a teacher in a classroom full of teenage girls, implies that women are not smart enough to do anything but the jobs of mother and wife. The wording could also be taken as offensive to women who do, in fact, have careers.

For the structured explanations, we observe that the model has relatively greater ability to identify the target and the targeting type in given statements compared to identifying the presence of abusive or negative elements such as violence, derogatory, profanity, obscenity, rejection, or accusation. One possible reason behind this is that target or targeting type is often directly referenced in the statement; often the answer of the target is identical to the vulgar terms a target is called in the statement. For example, instead of identifying female targets as women, or black targets as black, the model often generates 'target: b\*\*ch', or 'target: ni\*\*er' as the identification. Such identification is semantically correct but inappropriate. The relative lower performance in identifying violence, derogatory, profanity, obscenity, rejection, or accusation also reveals the model's limitations in grasping these given concepts---especially with nuanced concepts such as derogatory, profanity, or obscenity, compared to a more general concept such as 'insult'.

The capacity of GPT-3 to identify the target and targeting type in the structured approach can permit the model to detect problematic statements beyond the scope of sexism and racism. For example:

> Prompt:

> what a disgusting man and disgusting religion. He admits that muslim buy all their wifes ! .pff...This is why all muslims are poor...islam keeps them stupid !

> GPT-3 response:

> target: muslim | targeting type: religion ...

In the above example, GPT-3 successfully identified the target as a Muslim, and the targeting type as religion. With the flexibility of the approach, we have observe that GPT-3 could successfully identify the target identified based on not only gender and race, but also religion, sexual orientation, and disability. 

## Conclusion, weaknesses and next steps

In our study, we have found GPT-3 can generate coherent and meaningful explanations for why a given statement is sexist or racist in an open-ended manner. However, the reasoning is often false or only partial, and sometimes accompanied by made-up details. 

One particular issue with the human evaluation stage of the GPT-3 output is that there was just one evaluator. It is possible that this evaluator is biased in certain ways, and adding multiple evaluators would go part-way to addressing these concerns.

We caution the use of large language models in producing explanations of social themes at this stage, especially in open-ended settings. The model did not completely identify the abusive or negative elements in the given statements. But we see the structured approach as having potential to identify targets and targeting types beyond sexism or racism. In future studies, we intend to explore ways to improve GPT-3's performance in identifying abusive and negative elements in statements through the addition of instruction and the curation of the given examples.

\newpage

\appendix

# Appendix {-}

# Open-ended prompts

Prompt:

> Q: Is the following text in quotes sexist? Answer yes or no and explain why.
>
> '''If men shouldnt hit women because we are physically stronnger... then women shouldnt talk because they are fucking louder.'''
>
> A: 

GPT-3 response: 

> Yes this is sexist. This statement is using the stereotypical gender roles of men and women to justify why it is ok for men to hit women but it is not ok for women to speak. Men are seen as physically stronger and women are seen as the loud, screeching sex. This is a perfect example of the sexism in our society.

# Structured prompts

An example with eight attributes.

> '''What about a global massacre of men? Pussiezzz only girls'''
>
> [target: men | targeting type: gender| violence: yes | derogatory: yes | profanity: yes | obscenity: yes | rejection: no | accusation: no]
>
> '''kill all the african american monkeys'''
>
> [target: african american | targeting type: race| violence: yes | derogatory: yes | profanity: no | obscenity: no | rejection: no | accusation: no]
>
> '''Women are made for making babies and cooking dinner and nothing else!!!'''
>
> [target: women | targeting type: gender| violence: no | derogatory: yes | profanity: no | obscenity: no | rejection: no | accusation: no]
>
> '''The world would be better without white people because they stole from the black Egyptians in the before christ ages they was poor we wasnt whites  ruins everything for black people/White people have pig dicks and no shape no hips or no booty they have to get plastic surgery as us blacks get it from our mother whites brought skin cancer to america so learn some facts before you try to run us out america when we was here before yall racist ass smelly crackers and dont say you will beat my ass because you cant and if you wanna find out drop your adress'''
>
> [target: white | targeting type: race| violence: yes | derogatory: yes | profanity: yes | obscenity: no | rejection: yes | accusation: yes]
>
> '''You should know women's sports are a joke'''
>
> [

GPT-3 response: 

> [target: women's sports | targeting type: gender| violence: no | derogatory: yes | profanity: no | obscenities: no | rejection: no | accusation: no]

An example with six attributes.

> '''What about a global massacre of men? Pussiezzz only girls'''
>
> [target: men | targeting type: gender| violence: yes | insult: yes | rejection: no | accusation: no]
>
> '''kill all the african american monkeys'''
>
> [target: african american | targeting type: race| violence: yes | insult: yes | rejection: no | accusation: no]
>
> '''Women are made for making babies and cooking dinner and nothing else!!!'''
>
> [target: women | targeting type: gender| violence: no | insult: yes | rejection: no | accusation: no]
>
> '''The world would be better without white people because they stole from the black Egyptians in the before christ ages they was poor we wasnt whites  ruins everything for black people/White people have pig dicks and no shape no hips or no booty they have to get plastic surgery as us blacks get it from our mother whites brought skin cancer to america so learn some facts before you try to run us out america when we was here before yall racist ass smelly crackers and dont say you will beat my ass because you cant and if you wanna find out drop your adress'''
>
> [target: white | targeting type: race| violence: yes | insult: yes| rejection: yes | accusation: yes]
>
> '''You should know women's sports are a joke'''
>
> [

GPT-3 response: 

> [target: women's sports | targeting type: gender| violence: no | derogatory: yes | profanity: no | obscenities: no | rejection: no | accusation: no]

\newpage

# References
