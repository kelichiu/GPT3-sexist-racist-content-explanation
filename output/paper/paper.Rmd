---
output: 
  bookdown::pdf_document2:
    citation_package: natbib
    keep_tex: false
    toc: false
    fig_caption: true
    latex_engine: xelatex
    template: templates/svm-latex-ms.tex
bibliography: "references.bib"
header-includes:
  -  \usepackage{hyperref}
  -  \usepackage{amsmath}
biblio-style: apalike
title: "Explaining Sexism or Racism with GPT-3 (temp)"
thanks: "Code and data are available at: https://github.com/kelichiu/GPT3-sexist-racist-content-explanation. We gratefully acknowledge the support of Gillian Hadfield and the Schwartz Reisman Institute for Technology and Society. We thank Amy Farrow, Haoluan Chen, Mauricio Vargas Sepúlveda, and Tom Davidson for helpful suggestions. Comments on the `r format(Sys.time(), '%d %B %Y')` version of this paper are welcome at: rohan.alexander@utoronto.ca."
author:
- name: Ke-Li Chiu
  affiliation: University of Toronto
- name: Rohan Alexander
  affiliation: University of Toronto and Schwartz Reisman Institute
abstract: "abstract..."
keywords: "GPT-3; natural language processing; quantitative analysis; hate speech."
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 12pt
endnote: no
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), echo = FALSE, warning = FALSE)
library(dplyr)
library(tidyverse)
library(tidyr)
library(readr)
```

# Introduction

Large language models such as GPT-3 have the capacity to generate texts that are indistinguishable from human creation. In our previous study, we observed that GPT-3 the model's accuracy in detection sexist or racist text contents can be as high as 78 per cent [**cite our own paper**]. The importance of explanation in AI has been emphasized in numerous popular press outlets over the past decades [@mueller2019explanation]. In this study, we aim to assess GPT-3 ability to generate explanations on why given text contents are sexist or racist. We are interested in language models' ability in generating explanations because explanations play a crucial role in trust [@pieters2011explanation]. Trust is an essential factor for acceptance, which can shape the interaction between human and technology [@siau2018building]. 

Our approach to generate explanations is not to be mistaken from Explainable AI. Explainable AI is an approach that aims to  tackle the black box problem in AI decision making by making the decision making process auditable and transparent [**Citation**]. The explanations we prompt GPT-3 to produce are to be interpreted solely from a social lense, and are not shedding a light on the technical or algorithmic process of the generation. In other words, the model's decision on if a given text is sexist or racist does not depend on the explanation in generates. In fact, we have observed the model generating contradicting explanations to its own determination, discussed in the later section. 

However, such generations of explanations has an overlapping effect with Explainable AI — increasing users' trust in an AI system. @hoffman2018metrics state that various forms on intelligent systems are trusted more if their decisions are explained. Therefore, sophisticated language models' fidelity of generating valid and truthful explanations should be evaluated. 

In our study, we examine GPT-3's ability to generate explanations for why a given text is sexist or racist. We prompt GPT-3 to generate explanations in two approaches: open-ended and structured. In the open-ended method, GPT-3 is prompted to produce explanations on why a given text is sexist or racist in a question-answering manner. On the other hand, the model generates explanations following a given pattern under the structured method. We then assess the adequacy of the generated explanations to see if the model is fit for explaining why a given text is sexist or racist. 

# Background

## GPT-3

GPT-3 is the third generation of the GPT model created by OpenAI [@brown2020language]. Compared to the language models before GPT-3, GPT-3 is considered groundbreaking for a few reasons. First, GPT-3 is not specialized in one or just a few tasks, but a generalist that can be trained in doing all kinds of tasks. Second, it learns like human do. Given only a few examples, GPT-3 can pick up the patterns in the examples, and produce the results following the given patterns. Third, the examples and instructions fed to the model are expressed in natural language. In other words, it is programming with plain language instead of code. Lastly, the model generates the text quickly and fluently, and the generated contents are hard to be distinguished from human creation. Because of these powerful characteristics of this technology, researchers are wary that large language models will be used to generate misinformation at large scale [@bender2021dangers]. Besides GPT-3, large language models with competing scales such as Google G-Shard [**citation**] and G-Switch [**citation**] are as well not accessible to the public. As to date this paper is written, GPT-3 is not open to the public, and only selected users are granted the access through OpenAI API.


## Sexist and Racist Text Detection

This paper is, to some extent, an extension of our previous study on GPT-3's ability to detect sexist and racist textual contents [**Cite previous paper**]. In our previous paper, we use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 is able to identify sexist or racist text with an accuracy between 48 per cent and 69 per cent. With few-shot learning and an instruction included in the prompt, the model's accuracy can be as high as 78 per cent. We conclude that large language models have a role to play in hate speech detection, and that with further development language models could be used to counter hate speech and even self-police. We expand the research to explore the explanations of why a given text passage is sexist and racist in this paper.

## Comparison of Human- and Machine-Generated Explanations 

Using also GPT-3 as the source of AI-generated text contents, **Sharon et al citation** presented an unprecedented study that compares AI-generated explanations of subtle sexism assessment with human-creation. The authors collected a series of sexist text content from online forum, prompted GPT-3 to explain why a given text is sexist, then compared the AI-generated explanations with human creation. The authors have found that the model could often provide relevant explanations that are easily understood for human. They have also found that the themes of the explanations between AI and human are similar to each other. An important finding of **Sharon et al citation** is that GPT-3 "tends to take men's position more often than women's", which is an evidence of inherited bias in large language models. While the focus of **Sharon et al. citation** is sexism assessment, our study examines both sexism and racism assessment with both open-ended and structured approaches. In fact, the structured approach we proposed in this study could go beyond the scope of sexism or racism, but discrimination against disability, religions, appearances and more. 

# Methods

## Dataset: Ethos Hate Speech Dataset

We use the ETHOS dataset created by @mollas2020ethos. ETHOS is based on comments found in YouTube and Reddit. The ETHOS YouTube data is collected through Hatebusters [@anagnostou2018hatebusters]. Hatebusters is a platform that collects comments from YouTube and assigns a 'hate' score to them using a support vector machine. The Reddit data is collected from the Public Reddit Data Repository [@baumgartner2020pushshift]. The dataset has two variants: binary and multi-label. In the binary dataset comments are classified as hate or non-hate. In the multi-label variant, the comments are evaluated on measures that include violence, gender, race, ability, religion, and sexual orientation. The examples provided in this paper are from the ETHOS dataset and hence contain typos, misspelling, and offensive content.

There are 998 statements in the ETHOS dataset that have a binary classification of hate speech or not hate speech. Of these, the 433 statements that contain hate speech additionally have multiple labels that classify the content, for instance, does the comment have to do with violence, gender, race, nationality, disability, etc. We considered the **X** statements that contain race-based hate speech, and we focus on the **X** whose race-based score is at least 0.75. Similarly, we considered the **X** statements that contain gender-based hate speech, and again focused on the **X** whose gender-based score is at least 0.75. 

A human annotator evaluates the comments and extracts the following attributes from them — target, targeting type, violence, derogatory, profanity, obscenity, rejection, and accusation. Comments that have unclear target or targeting type are not included. The final dataset contains **X** comments. 

## Data Collection

The data is collected from prompting GPT-3 (Davinci model) in giving explanations through OpenAI API. We modified the following hyper-parameters for desired outputs — `temperature`, `stop`, `n`, and `max_tokens`. `temperature` determines how creative the response will be. The minimum value is 0 (conservative) and the maximum is 1 (creative). For open-ended responses, the temperature is set to 0.7 in our study for GPT-3 to obtain some creative liberty to generate open-ended responses. We set the `stop` parameter to "Q:", which means the generation will stop before the model generates another question, in case the model follows the pattern and attempts to generate another similar question after the answer. “n” is the number of responses generated, which is set to 5. 

Because GPT-3’s responses in an open-ended format can be very different each time, we prompt GPT-3 five times for each example to help us examine the variety of the responses and also increase the chance of getting a reasonable responses. “max_tokens”  is the maximum length of each response, which is set to 120. The goal of the prompt is to have the model answers "yes" or "no", then generates explanations within 120 words in free form. The prompts are formulated in a question-answering format. We start the prompt by “Q: ” and place the question in the following sentence to ask the model to explain why the provided example is sexist or not sexist. Following the question is the provided example wrapped in quotes. We then end the prompt with "A:", so the model will generate the response as an answer to the question. The example of an open-ended prompt can be found in **Appendix X**. This process is repeated until all the examples are given an explanation of why it is sexist or not, and the total number of collected responses in open-ended format is **X**.

For structured responses, the prompt contains two examples of sexists texts and two examples of racist texts. The examples demonstrate the desired patterns we want GPT-3 to follow, which is answering "yes" or "no" to a number of preset attributes under a given text. The attributes are target, targeting type, derogatory, profanity, obscenity, violence, rejection, and accusation. We aim to evaluate GPT-3's ability to identify who the target is, what they're targeted based on, and the presence of the characteristics of abusive content. The example of the structured prompt can be found in **Appendix X**. The temperature is iterated through five different values — 0, 0.25, 0.5, 0.75, and 1, so we can observe the performance of the model under different temperature. The "stop" parameter is set to "]", which is a closing bracket corresponding to the opening bracket in the prompt signaling the end of a generation. For each given text, we ask the model to generate 30 responses, and the total number of collected responses in structured format is **X**. @chen2012detecting state that offensive languages contain pejoratives, profanities, or obscenities. Therefore, presence of "insult" can be seen as an attributes that encompasses "derogatory", "profanity" and "obscenity". We then repeated the process with "derogatory", "profanity" and "obscenity" merged into "insult" to compare GPT-3's ability in detecting granular attributes versus a general attribute. The example of the structured prompt with reduced attributes can be found in **Appendix X**.

## Human Evaluation

The model-generated responses are then evaluated by a human annotator. The open-ended responses with three attributes — target identification, presence of made-up details, and reasoning adequacy. Target identification and presence of made-up details are binary evaluation that simply indicate if the model has identified the target of the sexist or racist content or has made up details in the generated explanations. The reasoning adequacy of each explanations are categorized as follows: no explanation, false explanation, insufficient explanation, complete explanation, and complete explanation with over-reaching explanation. 

For the structured responses, the human annotator evaluates the given texts by the same set of attributes — target, targeting type, violence, derogatory, profanity, obscenity, rejection, and accusation. The guidelines for labeling are as follows:

  * Target: the annotator is free to list none, one, more multiple targets based on their judgment. Although the examples are focus on targets whose identities are based on gender or race, the annotator is not limited to these options. 

  * Targeting type: similar to identifying targets, the annotator is also free to label the targeting type beyond to scope of gender or race.
  
  * Violence: The human annotator will only flag the presence of violence if the given text is inciting violence towards the target. Mentioning of a violent event is not considered presence of violence. 
  
  * Derogatory: **Belittling, diminishing**  
  
  * Profanity: If the given text contains any swear words or curse words. For example
  
  * Obscenity: **contains depiction of nudity, sex, or excretion is patently offensive and without artistic or scientific value.**
  
  * Rejection: The human annotator will flag the presence of rejection if the given text contains the author's explicit expression of not wanting the presence or existence of a target. 
  
  * Accusation: The author's accuses its target of misconducts of mischievous behaviours with generalization without evidence. For example, ""

# Results

## Open-Ended Responses

### Binary results

Before the generation of an explanation, GPT-3 is asked to identify first if a given text is sexist or racist by answering "yes" or "no". The following table shows matches and mismatches of such identification. If GPT-3 answers "yes", or in any other way to indicate a text is sexist or racist, the answer is a match, regardless the adequacy of the explanation. The number of samples are the aggregated counts of the five explanation of each comment. The results shown in Table X indicate that the majority of times GPT-3 did not successfully identify sexist or racist text contents. 

```{r binaryresultagg}
open_ended_results_raw <- read.csv(here::here("output/data/open_ended_results_with_human_label.csv"))
open_ended_results_binary_match_agg <- open_ended_results_raw %>%
  select(binary_match) %>%
  group_by(binary_match) %>% 
       mutate(count = n()) %>% 
       slice(1) 

open_ended_results_binary_match_agg <- open_ended_results_binary_match_agg %>% 
  mutate(binary_match = factor(binary_match)) %>% 
  mutate("Binary Match" = ifelse(binary_match == 1, "Match", "Mistmatch"))
open_ended_results_binary_match_agg <- open_ended_results_binary_match_agg[ , c("Binary Match", "count")] 
colnames(open_ended_results_binary_match_agg) <- c("Binary Match", "Count")
open_ended_results_binary_match_agg %>% 
  knitr::kable(caption = "Open-ended result aggregated binary match count, n = 76x5 = 380",
               booktabs = TRUE)
```

In each set of the five generated outputs to one given text, we count the number of times where GPT-3 successfully identify the given text as sexist or racist. If in three or more out of the five times the model has successful identification with one given example, it is marked as `majority correct`. The results of such the `majority correct` count is displayed in Table X. In 37 out of the 76 cases, the model has been right more frequently than wrong in identifying sexist or racist text. 

```{r binaryresult}
open_ended_results_binary_match <- open_ended_results_raw %>% 
  select(binary_match, comment) %>% 
  group_by(comment, binary_match) %>% 
       mutate(count = n()) %>% 
       slice(1) %>% 
       mutate(majority_correct = case_when(
                                binary_match == 0 & count > 2 ~ 0, 
                                binary_match == 0 & count < 3 ~ 1,
                                binary_match == 1 & count > 2 ~ 1, 
                                binary_match == 1 & count < 3 ~ 0)
              ) %>% 
       subset(select = -c(binary_match, count))%>% 
       unique()%>% 
       subset(select = -c(comment))%>% 
       group_by(majority_correct)%>% 
       mutate(count = n()) %>% 
       slice(1) 


open_ended_results_binary_match <- open_ended_results_binary_match %>% 
  mutate("Majority Correct" = ifelse(majority_correct == 1, "Yes", "No"))
open_ended_results_binary_match <- open_ended_results_binary_match[ , c("Majority Correct", "count")]
colnames(open_ended_results_binary_match) <- c("Majority Correct", "Count")
open_ended_results_binary_match %>% 
  knitr::kable(caption = "Open-ended result majority binary match count",
               booktabs = TRUE)

```

### Reasoning Adequacy

For the 180 generations where GPT-3 successfully identify the sexist or racist text, we further examine the adequacy of the generated explanations. Table X showcases the count of the adequacy labels of the 180 evaluations. In 76 out of 180 times, the model's explanations are constructed with false reasoning. In 60 cases, the model generated explanations with complete reasoning; in 5 cases, the model generated explanation with completed reasoning accompanied with overreaching reasoning. In 25 cases, the model produced explanations with insufficient reasoning; in 14 cases the model did not produce any explanation or the produced contents are not aimed to provide any explanation. 

```{r reasoningAdequacy}
open_ended_results_reason <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(reasoning) %>%
  mutate(count = n()) %>% 
  slice(1) %>% 
  select(reasoning, count)

colnames(open_ended_results_reason) <- c("Reasoning Adequacy", "Count")
open_ended_results_reason %>% 
  arrange(desc(Count)) %>% 
  knitr::kable(caption = "Open-ended result reasoning level counts",
               booktabs = TRUE)
```

### Target Identification and Made-Up Details

We delved into the 180 generations where GPT-3 successfully identify the sexist or racist text to see if the explanation provide an identification of who the target is and if the explanation is accompanied with made-up details. The results of target identification is displayed in Table X. 117 out of the 180 cases, the model provided an explanation along with the identification of the target (women, African Americans... etc). In some cases, the model generated explanations along with made-up details such as assigning false source to the given text. The number of cases where made-up details are found is shown in Table X. Nearly one third of the times, the model generated made-up details in the explanations. 

```{r openendedResultsTargetID}
open_ended_results_targetID <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(identify_target) %>%
  mutate(count = n()) %>% 
  slice(1) %>% 
  select(identify_target, count)

open_ended_results_targetID <- open_ended_results_targetID %>% 
  mutate("Target Identified" = ifelse(identify_target == 1, "Yes", "No"))
open_ended_results_targetID <- open_ended_results_targetID[ , c("Target Identified", "count")]
colnames(open_ended_results_targetID) <- c("Target Identified", "Count")
open_ended_results_targetID %>% 
  knitr::kable(caption = "Open-ended result target identification counts",
               booktabs = TRUE)
```

```{r openendedResultsMadeupDetails}
open_ended_results_madeup_details <- open_ended_results_raw %>%
  filter(binary_match == 1) %>%
  group_by(madeup_details) %>%
  mutate(count = n()) %>% 
  slice(1) %>% 
  select(madeup_details, count)

open_ended_results_madeup_details %>% 
  knitr::kable(caption = "Open-ended result made-up details counts",
               booktabs = TRUE)
```

## Structured Responses

30 generations per comment. 

```{r}
structured_a8_t0_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp0_attr8_n30.csv"))
structured_a8_t25_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp25_attr8_n30.csv"))
structured_a8_t50_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp50_attr8_n30.csv"))
structured_a8_t75_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp75_attr8_n30.csv"))
structured_a8_t100_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp1_attr8_n30.csv"))
structured_a6_t0_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp0_attr6_n30.csv"))
structured_a6_t25_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp25_attr6_n30.csv"))
structured_a6_t50_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp50_attr6_n30.csv"))
structured_a6_t75_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp75_attr6_n30.csv"))
structured_a6_t100_n30 <- read.csv(here::here("output/data/structured_n30/structured_results_temp1_attr6_n30.csv"))

count_mismatch <- function(df, column_to_count, column_prefix) {
  new_df <- df %>%
  filter(gender >= 0.75 | race >= 0.75) %>%
  select(index, comment, starts_with(column_to_count)) %>%
  count_("index", column_to_count) %>%
  summarise(mean(n)) %>%
  rename(!!column_prefix := "mean(n)" )
  new_df
}
```

```{r mismatchSummary}
### Target 
target_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_target", "Temperature 0")
target_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_target", "Temperature 25")
target_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_target", "Temperature 50")
target_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_target", "Temperature 75")
target_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_target", "Temperature 100")
target_mismatch_summary <- add_column(Attribute = "Target", target_a8_t0,target_a8_t25,target_a8_t50,target_a8_t75,target_a8_t100)
target_mismatch_summary <- target_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(target_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Targeting Type
targetingtype_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_targeting_type", "Temperature 0")
targetingtype_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_targeting_type", "Temperature 25")
targetingtype_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_targeting_type", "Temperature 50")
targetingtype_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_targeting_type", "Temperature 75")
targetingtype_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_targeting_type", "Temperature 100")
targetingtype_mismatch_summary <- add_column(Attribute = "Targeting Type",targetingtype_a8_t0,targetingtype_a8_t25,targetingtype_a8_t50,targetingtype_a8_t75,targetingtype_a8_t100)
targetingtype_mismatch_summary <- targetingtype_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(targetingtype_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Violence
violence_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_violence", "Temperature 0")
violence_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_violence", "Temperature 25")
violence_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_violence", "Temperature 50")
violence_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_violence", "Temperature 75")
violence_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_violence", "Temperature 100")
violence_mismatch_summary <- add_column(Attribute = "Violence", violence_a8_t0,violence_a8_t25,violence_a8_t50,violence_a8_t75,violence_a8_t100)
violence_mismatch_summary <- violence_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(violence_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Derogatory
derogatory_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_derogatory", "Temperature 0")
derogatory_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_derogatory", "Temperature 25")
derogatory_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_derogatory", "Temperature 50")
derogatory_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_derogatory", "Temperature 75")
derogatory_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_derogatory", "Temperature 100")
derogatory_mismatch_summary <- add_column(Attribute = "Derogatory", derogatory_a8_t0,derogatory_a8_t25,derogatory_a8_t50,derogatory_a8_t75,derogatory_a8_t100)
derogatory_mismatch_summary <- derogatory_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(derogatory_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Profanity
profanity_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_profanity", "Temperature 0")
profanity_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_profanity", "Temperature 25")
profanity_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_profanity", "Temperature 50")
profanity_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_profanity", "Temperature 75")
profanity_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_profanity", "Temperature 100")
profanity_mismatch_summary <- add_column(Attribute = "Profanity", profanity_a8_t0,profanity_a8_t25,profanity_a8_t50,profanity_a8_t75,profanity_a8_t100)
profanity_mismatch_summary <- profanity_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(profanity_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Obscenity
obscenity_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_obscenity", "Temperature 0")
obscenity_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_obscenity", "Temperature 25")
obscenity_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_obscenity", "Temperature 50")
obscenity_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_obscenity", "Temperature 75")
obscenity_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_obscenity", "Temperature 100")
obscenity_mismatch_summary <- add_column(Attribute = "Obscenity", obscenity_a8_t0,obscenity_a8_t25,obscenity_a8_t50,obscenity_a8_t75,obscenity_a8_t100)
obscenity_mismatch_summary <- obscenity_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(obscenity_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Rejection
rejection_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_rejection", "Temperature 0")
rejection_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_rejection", "Temperature 25")
rejection_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_rejection", "Temperature 50")
rejection_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_rejection", "Temperature 75")
rejection_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_rejection", "Temperature 100")
rejection_mismatch_summary <- add_column(Attribute = "Rejection", rejection_a8_t0,rejection_a8_t25,rejection_a8_t50,rejection_a8_t75,rejection_a8_t100)
rejection_mismatch_summary <- rejection_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(rejection_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")

### Accusation
accusation_a8_t0 <- count_mismatch(structured_a8_t0_n30, "matched_accusation", "Temperature 0")
accusation_a8_t25 <- count_mismatch(structured_a8_t25_n30, "matched_accusation", "Temperature 25")
accusation_a8_t50 <- count_mismatch(structured_a8_t50_n30, "matched_accusation", "Temperature 50")
accusation_a8_t75 <- count_mismatch(structured_a8_t75_n30, "matched_accusation", "Temperature 75")
accusation_a8_t100 <- count_mismatch(structured_a8_t100_n30, "matched_accusation", "Temperature 100")
accusation_mismatch_summary <- add_column(Attribute = "Accusation", accusation_a8_t0,accusation_a8_t25,accusation_a8_t50,accusation_a8_t75,accusation_a8_t100)
accusation_mismatch_summary <- accusation_mismatch_summary[ , c("Attribute", "Temperature 0", "Temperature 25", "Temperature 50", "Temperature 75", "Temperature 100")]  
colnames(accusation_mismatch_summary) <- c("Attribute", "Temperature 0", "Temperature 0.25", "Temperature 0.5", "Temperature 0.75", "Temperature 1")
```

```{r}
merge_df_30n <- function(df, temp) {
  new_df <- df %>%
  filter(gender >= 0.75 | race >= 0.75) %>%
  group_by(index) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE)  %>%
  select(index, starts_with("matched"))%>%
  mutate(temperature = temp) 
  new_df
}
structured_a8_t0_n30_list <- merge_df_30n(structured_a8_t0_n30, 0)
structured_a8_t25_n30_list <- merge_df_30n(structured_a8_t25_n30, 0.25)
structured_a8_t50_n30_list <- merge_df_30n(structured_a8_t50_n30, 0.5)
structured_a8_t75_n30_list <- merge_df_30n(structured_a8_t75_n30, 0.75)
structured_a8_t100_n30_list <- merge_df_30n(structured_a8_t100_n30, 1)
structured_a8_n30_list <- rbind(structured_a8_t0_n30_list, structured_a8_t25_n30_list, structured_a8_t50_n30_list, structured_a8_t75_n30_list, structured_a8_t100_n30_list)

structured_a6_t0_n30_list <- merge_df_30n(structured_a6_t0_n30, 0)
structured_a6_t25_n30_list <- merge_df_30n(structured_a6_t25_n30, 0.25)
structured_a6_t50_n30_list <- merge_df_30n(structured_a6_t50_n30, 0.5)
structured_a6_t75_n30_list <- merge_df_30n(structured_a6_t75_n30, 0.75)
structured_a6_t100_n30_list <- merge_df_30n(structured_a6_t100_n30, 1)
structured_a6_n30_list <- rbind(structured_a6_t0_n30_list, structured_a6_t25_n30_list, structured_a6_t50_n30_list, structured_a6_t75_n30_list, structured_a6_t100_n30_list)
```

### Number of Matched Numbers within Different Temperatures 

Figure X shows the structured result average number of attribute matches in temperature 0, 0.25, 0.5, 0.75 and 1. We observe a general trend of number of matches decreasing as the temperature increases. Except for 'Target', 'Profanity' and 'Obscenity' where the model performed the best under temperature 0.25 by a thin margin, the results of temperature 0 have the highest number of matches with human evaluation in every other attribute. The model performed consistently the poorest under temperature 1. Moreover, as the distribution of matched number varies within wider ranges as the temperature increases.  

```{r, fig.height=7}
p1 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_target))
p1 <- p1 + geom_boxplot()

p2 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_targeting_type))
p2 <- p2 + geom_boxplot()

p3 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_violence))
p3 <- p3 + geom_boxplot()

p4 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_derogatory))
p4 <- p4 + geom_boxplot()

p5 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_profanity))
p5<- p5 + geom_boxplot()

p6 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_obscenity))
p6<- p6 + geom_boxplot()

p7 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_rejection))
p7<- p7 + geom_boxplot()

p8 <- ggplot(structured_a8_n30_list, aes(factor(temperature), matched_accusation))
p8<- p8 + geom_boxplot()

gridExtra::grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8, ncol=2)
```

### Comparison of All Attributes

The descriptive statistic is conducted and shown in Table X to assess the model' overall performances in each attribute. The results show that the model performs the best in identifying the identity categories (gender, race...etc) a sexist or racist text is targeting (sex, gender, race, origin...etc). Second to the targeting type, the model also performs well in identifying who the target is (women, African Americans...etc). The model seems to have varied capabilities in identifying the presence of violence, derogatory, profanity, obscenity, rejection, and accusation in the texts. 

```{r matchSummaryTable}
all_attr_summary <- add_row(target_mismatch_summary,targetingtype_mismatch_summary) %>%
  add_row(violence_mismatch_summary) %>%
  add_row(derogatory_mismatch_summary)%>%
  add_row(profanity_mismatch_summary)%>%
  add_row(obscenity_mismatch_summary)%>%
  add_row(rejection_mismatch_summary)%>%
  add_row(accusation_mismatch_summary)
colnames(all_attr_summary) <- c("Attribute", "Temp 0", "Temp 0.25", "Temp 0.5", "Temp 0.75", "Temp 1")
rownames(all_attr_summary) <- all_attr_summary$Attribute

all_attr_summary_t <- t(all_attr_summary)

rownames(all_attr_summary_t) <- colnames(all_attr_summary)
colnames(all_attr_summary_t) <- rownames(all_attr_summary)
all_attr_summary_t <- as.data.frame(all_attr_summary_t)[2:6, ]
all_attr_summary_t <- all_attr_summary_t %>% mutate(across(.fns = type.convert))

summary(all_attr_summary_t) %>% 
  knitr::kable(caption = "Structured results attribute matches descriptive data",
               booktabs = TRUE)
```

### Number of Matched Numbers within Granular Attributes V.S. General Attribute

We define ‘Insult’ as a general attribute that encompasses ‘Derogatory’, ‘Profanity’, and ‘Obscenity’, which are what we called granular attributes. We examine how GPT-3 performs in identifying a more general attribute compared to identifying more granular attributes. Figure X shows the average numbers of matches in ‘Insult’, ‘Derogatory’, ‘Profanity’, and ‘Obscenity’ in temperature 0, 0.25, 0.5, 0.75 and 1. The results reveals that the model performs better with identifying general attributes compared to granular attributes. 

```{r insultMismatchSummaryLineChart, fig.height=3}
insult_df <- add_column(structured_a6_n30_list["index"],structured_a6_n30_list["temperature"], structured_a6_n30_list["matched_insult"], structured_a8_n30_list["matched_derogatory"], structured_a8_n30_list["matched_profanity"],  structured_a8_n30_list["matched_obscenity"])
insult_df <- insult_df %>%
  group_by(index, temperature) %>%
  summarise_if(is.numeric, sum, na.rm = TRUE) %>%
  group_by(temperature) %>%
  select(-index)%>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  pivot_longer(!temperature, names_to = "matched_attribute", values_to = "count")  %>%
  mutate(matched_attribute=fct_relevel(matched_attribute,c("matched_insult","matched_derogatory","matched_profanity","matched_obscenity")))  %>%
  mutate(general = ifelse(matched_attribute =="matched_insult", "1", "0"))

values = c( c(matched_insult="orange"), "#a0b9e3", "#4c82df", "#0c089c")

ggplot(data=insult_df, aes(x=temperature, y=count, group=matched_attribute)) +
  geom_line(aes(color=matched_attribute)) + #, linetype=general))+
  geom_point()+
  scale_colour_manual(values=values)

```

# Discussion

* Ability to generate open-Ended explanations is sub-optimal

* Open-ended explanations are often accompanied with made-up details

    **Example of a generation of made-up detail**


* Structured explanations identify higher-level attributes better compared to granular attributes 

* Structured explanations can go beyond the scope of sexism and racism 

This approach actually permits the model to detect problematic texts beyond the scope of sexism and racism, because now, the targets can be of any groups identified based on not only gender and race, but also religion, sexual orientation, and disability. **Examples of discrimination based on disability, religion, and appearance**

**Why this is not Explainable AI**

Explainable AI is an approach that’s been very popular to tackle the black box problem in AI decision making. Often times, we have hard time to understand why AI makes the decision it makes, and there’s no way to deconstruct it. Explainable AI is the approach of that decision making process auditable and transparent. Why we do is not explainable AI because the explanations on why a text passage is sexist or racist provided by GPT-3 are from a social lense, and are not shedding a light on any technical or system process for the output. In other words, the explanations displayed are not the rationale GPT-3 made the decisions based on. GPT-3 is saying that the text is sexist or racist because of the math behind the model, not because of the explanations it generated. 

    **The example of contradicting explanation**

\newpage

\appendix

# Appendix {-}

## Open-Ended Prompts

Prompt:

> Q: Is the following text in quotes sexist? Answer yes or no and explain why.
>
> '''If men shouldnt hit women because we are physically stronnger... then women shouldnt talk because they are fucking louder.'''
>
> A: 

GPT-3 response: 

> Yes this is sexist. This statement is using the stereotypical gender roles of men and women to justify why it is ok for men to hit women but it is not ok for women to speak. Men are seen as physically stronger and women are seen as the loud, screeching sex. This is a perfect example of the sexism in our society.

## Structured Prompts

**Eight Attributes**

> '''What about a global massacre of men? Pussiezzz only girls'''
>
> [target: men | targeting type: gender| violence: yes | derogatory: yes | profanity: yes | obscenity: yes | rejection: no | accusation: no]
>
> '''kill all the african american monkeys'''
>
> [target: african american | targeting type: race| violence: yes | derogatory: yes | profanity: no | obscenity: no | rejection: no | accusation: no]
>
> '''Women are made for making babies and cooking dinner and nothing else!!!'''
>
> [target: women | targeting type: gender| violence: no | derogatory: yes | profanity: no | obscenity: no | rejection: no | accusation: no]
>
> '''The world would be better without white people because they stole from the black Egyptians in the before christ ages they was poor we wasnt whites  ruins everything for black people/White people have pig dicks and no shape no hips or no booty they have to get plastic surgery as us blacks get it from our mother whites brought skin cancer to america so learn some facts before you try to run us out america when we was here before yall racist ass smelly crackers and dont say you will beat my ass because you cant and if you wanna find out drop your adress‚'''
>
> [target: white | targeting type: race| violence: yes | derogatory: yes | profanity: yes | obscenity: no | rejection: yes | accusation: yes]
>
> '''You should know women's sports are a joke'''
>
> [

GPT-3 response: 

> target: women's sports | targeting type: gender| violence: no | derogatory: yes | profanity: no | obscenities: no | rejection: no | accusation: no

**Six Attributes**

> '''What about a global massacre of men? Pussiezzz only girls'''
>
> [target: men | targeting type: gender| violence: yes | insult: yes | rejection: no | accusation: no]
>
> '''kill all the african american monkeys'''
>
> [target: african american | targeting type: race| violence: yes | insult: yes | rejection: no | accusation: no]
>
> '''Women are made for making babies and cooking dinner and nothing else!!!'''
>
> [target: women | targeting type: gender| violence: no | insult: yes | rejection: no | accusation: no]
>
> '''The world would be better without white people because they stole from the black Egyptians in the before christ ages they was poor we wasnt whites  ruins everything for black people/White people have pig dicks and no shape no hips or no booty they have to get plastic surgery as us blacks get it from our mother whites brought skin cancer to america so learn some facts before you try to run us out america when we was here before yall racist ass smelly crackers and dont say you will beat my ass because you cant and if you wanna find out drop your adress‚'''
>
> [target: white | targeting type: race| violence: yes | insult: yes| rejection: yes | accusation: yes]
>
> '''You should know women's sports are a joke'''
>
> [

GPT-3 response: 

> target: women's sports | targeting type: gender| violence: no | derogatory: yes | profanity: no | obscenities: no | rejection: no | accusation: no

\newpage

# References
